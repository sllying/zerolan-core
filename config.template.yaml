# Modify your config for models you want to use.

ASR:
  id: "iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11001 # Port of the API service you want to start  
  config:
    iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1:
      model_path: "iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1" # Directory of the model
      chunk_size: [ 0, 10, 5 ]  # [0, 10, 5] 600ms, [0, 8, 4] 480ms
      encoder_chunk_look_back: 4  # Number of chunks to lookback for encoder self-attention
      decoder_chunk_look_back: 1  # Number of encoder chunks to lookback for decoder cross-attention
      version: "v2.0.4" # Paraformer version
      chunk_stride: 9600  # chunk_size[1] * 960
    kotoba-tech/kotoba-whisper-v2.0:
      model_path: "kotoba-tech/kotoba-whisper-v2.0"
      device: "cuda"
    Qwen/Qwen3-ASR-1.7B-transformers:
      model_path: "Qwen/Qwen3-ASR-1.7B"
      device_map: "cuda:0"
      dtype: "bfloat16"
      max_inference_batch_size: 8
      max_new_tokens: 256
      attn_implementation: null
      forced_aligner: null
      forced_aligner_device_map: "cuda:0"
      forced_aligner_dtype: "bfloat16"
    Qwen/Qwen3-ASR-1.7B-vllm:
      model_path: "Qwen/Qwen3-ASR-1.7B"
      gpu_memory_utilization: 0.7
      max_inference_batch_size: 128
      max_new_tokens: 4096
      forced_aligner: null
      forced_aligner_device_map: "cuda:0"
      forced_aligner_dtype: "bfloat16"

LLM:
  id: "THUDM/chatglm3-6b" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11002 # Port of the API service you want to start
  config:
    THUDM/glm-4-9b-chat-hf:
      model_path: "THUDM/glm-4-9b-chat-hf"
      device: "cuda"
      max_length: 5000
    THUDM/chatglm3-6b:
      model_path: "THUDM/chatglm3-6b" # Directory of the model
      quantize: null  # null | 4 | 8
      device: "cuda" # Device to run the model
    Qwen/Qwen-7B-Chat:
      model_path: "Qwen/Qwen-7B-Chat" # Directory of the model
      quantize: null  # null | 4 | 8
      device: "cuda" # Device to run the model
      precise: null # "bf16" | "fp16"
    augmxnt/shisa-7b-v1:
      model_path: "augmxnt/shisa-7b-v1" # Directory of the model
      device: "cuda" # Device to run the model
    01-ai/Yi-6B-Chat:
      model_path: "01-ai/Yi-6B-Chat" # Directory of the model
      device: "cuda" # Device to run the model
    deepseek-ai/DeepSeek-R1-Distill-Llama-8B:
      model_path: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" # Directory of the model
      max_length: 23000
      tensor_parallel_size: 1
    deepseek-ai/DeepSeek-R1-Distill-Qwen-14B:
      model_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" # Directory of the model
      max_length: 23000
      tensor_parallel_size: 2
    ollama:
      api_url: "http://localhost:11434"  # Ollama API URL
      model_name: "gemma3:1b"  # Ollama model name (e.g., llama2, mistral, qwen, deepseek-r1, etc.)
      temperature: 0.7
      top_p: 0.95
      timeout: 60

ImgCap:
  id: "Salesforce/blip-image-captioning-large" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11003 # Port of the API service you want to start  
  config:
    Salesforce/blip-image-captioning-large:
      model_path: "Salesforce/blip-image-captioning-large" # Directory of the model
      device: "cuda" # Device to run the model

OCR:
  id: "paddlepaddle/PaddleOCR" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11004 # Port of the API service you want to start
  config:
    paddlepaddle/PaddleOCR:
      model_path: "paddlepaddle/PaddleOCR" # Directory of the model
      lang: "ch" # Language ["ch", "en", "fr", "german", "korean", "japan"]

TTS:
  id: "AkagawaTsurunaki/GPT-SoVITS" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11005 # Port of the API service you want to start
  config:
    AkagawaTsurunaki/GPT-SoVITS:
    # Under developing for quick installing

VLA:
  ShowUI:
    id: "showlab/ShowUI-2B"
    host: "0.0.0.0"
    port: 11009
    config:
      min_pixels: 200704 # 256*28*28
      max_pixels: 1053696 # 1344*28*28
      showui_model_path: "showlab/ShowUI-2B" # Directory of the model
      showui_model_device: auto # Device to run the model
      qwen_vl_2b_instruct_model_path: "Qwen/Qwen2-VL-2B-Instruct" # Directory of the model

database:
  milvus:
    config:
      host: "0.0.0.0" # Host of the API service you want to start
      port: 11010 # Port of the API service you want to start
      db_path: "data/milvus.db" # Path of the Milvus database to save

VidCap:
  id: "iic/multi-modal_hitea_video-captioning_base_en" # The model you want to use
  host: "0.0.0.0" # Host of the API service you want to start
  port: 11011 # Port of the API service you want to start
  config:
    iic/multi-modal_hitea_video-captioning_base_en:
      model_path: "iic/multi-modal_hitea_video-captioning_base_en" # Directory of the model
